\documentclass[11pt,a4paper]{article}

% Essential math packages
\usepackage{amsmath,amssymb,amsfonts,amsthm}

% Additional useful packages
\usepackage{graphicx}
\usepackage{physics}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{microtype}

% Wide page with narrow margins
\usepackage[margin=1in]{geometry}

% Bibliography setup
\usepackage[
  backend=bibtex,
  style=numeric,
  sorting=none,
  maxbibnames=99
]{biblatex}
\addbibresource{jaynes-world.bib}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\title{\LARGE \textbf{Jaynes World}}
\author{Neil D. Lawrence}
\date{March 7, 2025}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

In this paper we introduce Jaynes' world. Jaynes world is inspired by the work of John Conway in creating the \emph{Game of Life} and Stephen Wolfram in creating a series of one dimensional cellular automata. Like those worlds, the aim is to create a world where complex patterns can emerge from simple phenomena. 

\subsection{Gradient Descent}

We would like the world to be understandable by a typical high school physicist. We will therefore base the world on concepts that \emph{could} be understood at that level. But at its core will be one of the most mysterious concepts in science, entropy. 

Jaynes' world proceeds by following the trajectory of steepest ascent in entropy. The name of the world is inspired by Edwin Jaynes who was a great proponent of the principle ofmaximum entropy.

Like Conway's Game of Life and other cellular automata, our world will be defined on a grid. The grid consists of cells with unit length sides. The unit length gives the minimum distance that allows us to differentiate information in this universe. 

A fundamental parameter of Jaynes' world is given by the final maximum entropy configuration. In Jaynes' world we use Shannon's entropy which is defined in bits. We define the maximum entropy configuration to contain $N$ bits, where one bit of information is gained from knowing the outcome of a 50\% probability like a coin toss. This also leads to an information conservation law. The number of information carrying cells in the world remains fixed. Cells which don't contain information are said to be in a `vacuum' state.  

Conway used two dimensions to define his world, Wolfram focussed on one. Keeping dimensions low is one way of ensuring an aspect of simplicity in the world.

We go in the other direction to define simplicity, instead of specifying a number of dimensions we assume our world is non-parametric. From a practical sense this means there are as many dimensions as we need. For our world, that means we assume that there are $N$ dimensions. Because there is nothing we can do in $\infty$ dimensions that we can't do in $N$ dimensions.  The bits of information are stored in coordinates given by an $N$ dimensional vector $\mathbf{x}$ of integer values which specify cell locations in the grid.

We can represent the non-parametric nature of the world by introducing another $N$-dimensional vector $\boldsymbol{\theta}$, that define how many cells there are in each dimension. We define,
\[
\theta_i = \log_2 \ell_i,
\]
where $\ell_i$ is the width of the grid in the $i$th dimension. 

To add further complexity, we assume information can only change when it moves. In each `turn' of the game it can only move into a neigbouring cell.  

\subsection{First Observation in Jaynes' World}

Already with these definitions we can make some observations about the initial configuration of Jaynes' world. Regardless where we initialise to a sophisticated observer\footnote{We have not yet defined observers, but for the moment we assume they might exist.} will perceive that their world's configuration emerged from an origin point where entropy was minimised. This would be a point with o vacuum where all values of \mathbf{x} are coupled implying they are neighbours. 

Note that observers would perceive this initial state regardless of the actual initial configuration of their world, because when they reversed iterations they would be descending the entropy towards the origin point which would always be a minimum entropy configuration. 

This gives us a necessary initial condition of Jaynes' world, a state of minimum entropy and maximum entropic potential where all bits are neighbouring each other, so we need to place our bits in an $N-1$ dimensional simplex. This state has an entropy of zero. 

The Jaynes' world game ends when the state of maximum entropy is reached. Maximum entropy distributions are characterised by a relationship between the Jacobian and the Hessian which comes to us from information geometry, the maximum entropy distribution is characterised by a Hessian which is the square of its Jacobian. 

The initial state of the world is at minimum entropy. But we will assume that the first gradeint step gives us the form of that probability distribtuion that maximises its entropy. In otherwords the first thing Jaynes' world does is give the intial minimal entropy configuration a probabilistic form through free-form optimisation of the variational distribution to its maximum entropy form, i.e. a distribution where the Hessian is the square of the Jacobian.

\subsection{Dynamics}

The update rule for Jayne's world is `steepest gradient ascent'. At each iteraction we make the move that vies us most entropy in th system. But this is not the optimal maximisation rule. Indeed, we will now conjecture that this update rule provides the slowest route to the top. 

Newton's method is an iterative method for finding roots of an equation. It can also be modified to find the stationary points of a function. When applied in multiple dimensions it is called a second order method. In these cases, instead of descending the gradient directly we change the parameters through a transformation  of the Jacobian. At each iteration we weight the Jacobian by the inverse Hessian. This second order descent methods converges faster than standard gradient methods, but it can be prohibitive to implement because of the cost of computing and inverting the Hessian. However, the condition on the dynamics that comes from our system's minima now comes to our aid.

If the Hessian is the square of the Jacobian, then the descent direction becomes the \emph{Moore-Penrose} pseudo-inverse of the systems Jacobian. So in otherwords, steepest descent is the inverse of the optimal descent direction. In Jaynes' world physics progresses by taking the `scenic route'. Conceptually what happens is as follows, instead of descending in the direction of the \emph{inverse Hessian}, we end up descending in the direction provided by the \emph{Hessian}. As this proceeds it means that we descend the `ridges' of the configuration space. Areas of the configuration space that are actually maxima in almost all projections of configuration space, but with a few `escape dimensions' that define the ridge. 

These ridges give they physics of Jaynes' world some predictability. But this predictability is interspersed by moments of decision. These occur when the inevitable stationary point is reached. THe stationary point will be a saddle point, and the Hessian will have negative eigenvalues. We reach these points because we are descending according to the slowest route, which means we are seeking out saddle points. In effect we are taking a tour of parameter space and avoiding the global minimum. The tour must proceed in a way that maximises entropy production, but also in a way that keeps the entropic potential of the world as high as possible. 

These dynamics present a rescipe for complexity, in Jaynes' world there's a paradox, we are inevitably seeking the directions that maximise entropy production, but we end up taking the slowest route possible to the final destination.\footnote{It is worth considering how machine learning might work in Jaynes' world. The trick would be to build models that start in regions of low entropic potential and climb the entropic gradient These models m}



\printbibliography

\end{document}
